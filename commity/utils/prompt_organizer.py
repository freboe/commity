import re
from typing import Final

from commity.utils.token_counter import count_tokens

# Constants
MAX_DIFF_LENGTH: Final[int] = 15000
MAX_FILES_IN_SUMMARY: Final[int] = 30
MAX_COMPRESSED_LINES: Final[int] = 1000
TOKEN_ESTIMATE_FACTOR: Final[int] = 4


def check_diff_length(diff_text, threshold=MAX_DIFF_LENGTH):
    if len(diff_text) > threshold:
        return (
            True,
            f"âš ï¸ Diff too long ({len(diff_text)} characters), it is recommended to submit in batches or simplify changesã€‚",
        )
    return False, ""


def generate_prompt_summary(diff_text):
    # æå–æ–‡ä»¶åå’Œä¿®æ”¹è¡Œæ•°ï¼ˆç¤ºä¾‹ç”¨ git diff ç»“æ„ï¼‰
    files = re.findall(r"diff --git a/(.+?) ", diff_text)
    summary = [f"- Change Fileï¼š{file}" for file in files[:MAX_FILES_IN_SUMMARY]]  # é™åˆ¶å‰Né¡¹
    return "ğŸ“ Change Summaryï¼š\n" + "\n".join(summary)


def compress_diff_to_bullets(diff_text, max_lines=MAX_COMPRESSED_LINES):
    lines = diff_text.splitlines()
    compressed = []

    for line in lines:
        if line.startswith("+") and not line.startswith("+++"):
            compressed.append(f"- Addï¼š{line[1:].strip()}")
        elif line.startswith("-") and not line.startswith("---"):
            compressed.append(f"- Deleteï¼š{line[1:].strip()}")

        if len(compressed) >= max_lines:
            # compressed.append("...å†…å®¹å·²æˆªæ–­")
            compressed.append("...<truncated>")
            break

    return "\n".join(compressed)


def summary_and_tokens_checker(
    diff_text: str, max_output_tokens: int, model_name: str, provider: str = "openai"
) -> str:
    """æ·»åŠ æ€»ç»“å’Œå‹ç¼©ç‰ˆæœ¬çš„diffï¼Œæ„å»ºæœ‰æ•ˆé•¿åº¦çš„tokensçš„æç¤ºè¯è¯­ï¼Œé¿å…è¿‡é•¿å¯¼è‡´æ¨¡å‹ç”Ÿæˆå¤±è´¥

    Args:
    ----
        diff_text: Git diff text to check
        max_output_tokens: Maximum tokens allowed
        model_name: Model name for token counting
        provider: LLM provider (openai, gemini, ollama, openrouter)

    Returns:
    -------
        Original or compressed diff text that fits within token limit

    """
    max_user_tokens = max_output_tokens * 1

    token_count = count_tokens(diff_text, model_name, provider)
    if token_count <= max_user_tokens:
        return diff_text

    _, warning_msg = check_diff_length(diff_text)
    prompt_summary = generate_prompt_summary(diff_text)
    compressed_diff = compress_diff_to_bullets(diff_text)

    original_prompt = f"{warning_msg}\n{prompt_summary}\n\nğŸ” Change details (compressed version)ï¼š\n{compressed_diff}"

    # æ„å»ºæœ€ç»ˆæç¤ºï¼Œä¼˜å…ˆä½¿ç”¨å‹ç¼©ç‰ˆæœ¬
    final_prompt = original_prompt

    # å†æ¬¡æ£€æŸ¥ token æ•°é‡ï¼Œå¦‚æœä»ç„¶è¿‡é•¿ï¼Œåˆ™è¿›ä¸€æ­¥æˆªæ–­
    if count_tokens(final_prompt, model_name, provider) > max_user_tokens:
        # ç®€å•æˆªæ–­ï¼Œä¿ç•™å¼€å¤´éƒ¨åˆ†
        # è®¡ç®—éœ€è¦æˆªæ–­çš„å­—ç¬¦æ•°
        current_tokens = count_tokens(final_prompt, model_name, provider)
        excess_tokens = current_tokens - max_user_tokens
        # ä¼°ç®—æ¯ä¸ª token å¯¹åº”çš„å­—ç¬¦æ•°ï¼Œè¿™é‡Œç®€å•å‡è®¾ä¸€ä¸ª token çº¦ç­‰äº 4 ä¸ªå­—ç¬¦ï¼ˆå¯¹äºè‹±æ–‡ï¼‰
        # è¿™æ˜¯ä¸€ä¸ªç²—ç•¥çš„ä¼°ç®—ï¼Œå®é™…åº”æ ¹æ®å…·ä½“æ¨¡å‹å’Œè¯­è¨€è°ƒæ•´
        chars_to_remove = excess_tokens * TOKEN_ESTIMATE_FACTOR

        if len(final_prompt) > chars_to_remove:
            final_prompt = final_prompt[: len(final_prompt) - chars_to_remove] + "\n...<truncated>"
        else:
            # å¦‚æœå‹ç¼©åçš„å†…å®¹ä»ç„¶å¤ªé•¿ï¼Œè¿”å› original_prompt è®©æ¨¡å‹å¤„ç†ï¼Œå¦‚æœæ¨¡å‹æŠ¥é”™åˆ™å±•ç¤ºé”™è¯¯ä¿¡æ¯
            return original_prompt

    return final_prompt
